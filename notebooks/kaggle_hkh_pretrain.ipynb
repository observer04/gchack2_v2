{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "933d92b9",
   "metadata": {},
   "source": [
    "# Glacier Segmentation - HKH Pretraining\n",
    "\n",
    "This notebook runs Phase 0: HKH dataset pretraining.\n",
    "\n",
    "**Expected runtime:** 2-3 hours on dual T4\n",
    "\n",
    "**Expected MCC:** 0.75-0.78\n",
    "\n",
    "**Output:** `weights/hkh_pretrained.pth` (~44 MB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf3db50",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6357d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/observer04/gchack2_v2.git\n",
    "%cd gchack2_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c178a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q segmentation-models-pytorch albumentations timm scikit-learn rasterio geopandas ttach scikit-image opencv-python PyYAML tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4533db6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU setup\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9096a7",
   "metadata": {},
   "source": [
    "## 2. Download Competition Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7f81ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download competition data\n",
    "!wget https://www.glacier-hack.in/train2.zip\n",
    "!unzip train2.zip -d /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa830e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize competition data\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "comp_data_dir = Path('/kaggle/working/gchack2_v2/data/competition')\n",
    "comp_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Move extracted data to proper location\n",
    "train_source = Path('/kaggle/working/Train')\n",
    "if train_source.exists():\n",
    "    for band_dir in train_source.iterdir():\n",
    "        dest = comp_data_dir / band_dir.name\n",
    "        if dest.exists():\n",
    "            shutil.rmtree(dest)\n",
    "        shutil.move(str(band_dir), str(dest))\n",
    "    print(\"✓ Competition data organized\")\n",
    "\n",
    "# Verify structure\n",
    "print(\"\\nData structure:\")\n",
    "for item in sorted(comp_data_dir.iterdir()):\n",
    "    if item.is_dir():\n",
    "        count = len(list(item.glob('*.tif')))\n",
    "        print(f\"  {item.name}: {count} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7fad05",
   "metadata": {},
   "source": [
    "## 3. Download HKH Dataset\n",
    "\n",
    "**Note:** HKH dataset is ~29.4 GB compressed.\n",
    "\n",
    "This may take 10-15 minutes on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c89f880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download HKH dataset (29.4 GB - includes patches, polygons, images)\n",
    "!mkdir -p /kaggle/working/gchack2_v2/data/hkh/raw\n",
    "%cd /kaggle/working/gchack2_v2/data/hkh/raw\n",
    "\n",
    "# Download from Azure (fastest for Kaggle)\n",
    "# Alternative mirrors: GCP or AWS (see KAGGLE_GUIDE.md)\n",
    "!wget -O hkh_patches.tar.gz https://lilawildlife.blob.core.windows.net/lila-wildlife/icimod-glacier-mapping/hkh_patches.tar.gz\n",
    "\n",
    "# Extract dataset\n",
    "!tar -xzf hkh_patches.tar.gz\n",
    "!ls -lh\n",
    "\n",
    "%cd /kaggle/working/gchack2_v2\n",
    "print(\"✓ HKH dataset downloaded (29.4 GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4605ab4f",
   "metadata": {},
   "source": [
    "## 4. Preprocess HKH Dataset\n",
    "\n",
    "Convert raw HKH data to 512×512 tiles suitable for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c65ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess HKH Dataset\n",
    "# HKH has 15 channels but competition has 5 - we need to extract matching bands\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# The extracted HKH data structure varies by download\n",
    "# Check what we actually got\n",
    "hkh_raw = Path('/kaggle/working/gchack2_v2/data/hkh/raw')\n",
    "print(\"Contents of HKH download:\")\n",
    "!ls -lh /kaggle/working/gchack2_v2/data/hkh/raw/\n",
    "\n",
    "# Create processed directories\n",
    "proc_dir = Path('/kaggle/working/gchack2_v2/data/hkh/processed')\n",
    "(proc_dir / 'images').mkdir(parents=True, exist_ok=True)\n",
    "(proc_dir / 'masks').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\n⚠️ PREPROCESSING REQUIRED:\")\n",
    "print(\"The HKH dataset uses numpy format with 15 channels.\")\n",
    "print(\"Our implementation (HKHDataset class) will:\")\n",
    "print(\"  - Automatically select 5 matching bands: [B1, B2, B3, B5, B6_high]\")\n",
    "print(\"  - Convert 2-channel masks to 4-class format\")\n",
    "print(\"  - Handle this during training - NO manual preprocessing needed!\")\n",
    "print(\"\\nReady to train with HKHDataset class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200230d0",
   "metadata": {},
   "source": [
    "## 5. Run HKH Pretraining\n",
    "\n",
    "Train Boundary-Aware U-Net on HKH dataset.\n",
    "\n",
    "**Expected time:** 2-3 hours (60 epochs × ~90 sec/epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca37a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "!python src/training/train.py \\\n",
    "    --config configs/hkh_pretrain_kaggle.yaml \\\n",
    "    --experiment_name hkh_pretrain_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41938c81",
   "metadata": {},
   "source": [
    "## 6. Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabce0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check final metrics\n",
    "import torch\n",
    "\n",
    "checkpoint = torch.load('weights/hkh_pretrain_v1/best_checkpoint.pth')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HKH Pretraining Results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "metrics = checkpoint['metrics']\n",
    "print(f\"\\nBest Epoch: {checkpoint['epoch']}\")\n",
    "print(f\"Validation MCC: {metrics['mcc']:.4f}\")\n",
    "print(f\"Mean IoU: {metrics['mean_iou']:.4f}\")\n",
    "print(f\"Macro F1: {metrics['macro_f1']:.4f}\")\n",
    "\n",
    "print(\"\\nPer-class IoU:\")\n",
    "class_names = ['Background', 'Glacier', 'Debris', 'Lake']\n",
    "for i, name in enumerate(class_names):\n",
    "    iou = metrics.get(f'class_{i}', 0)\n",
    "    print(f\"  {name:12}: {iou:.4f}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check file size\n",
    "import os\n",
    "size_mb = os.path.getsize('weights/hkh_pretrain_v1/best_checkpoint.pth') / 1e6\n",
    "print(f\"\\nCheckpoint size: {size_mb:.1f} MB\")\n",
    "\n",
    "# Success criteria\n",
    "print(\"\\n✓ Success Criteria:\")\n",
    "print(f\"  MCC ≥ 0.75: {'✓ PASS' if metrics['mcc'] >= 0.75 else '✗ FAIL'}\")\n",
    "print(f\"  IoU > 0.60: {'✓ PASS' if all(metrics.get(f'class_{i}', 0) > 0.60 for i in range(4)) else '✗ FAIL'}\")\n",
    "print(f\"  Size < 50MB: {'✓ PASS' if size_mb < 50 else '✗ FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b76c51f",
   "metadata": {},
   "source": [
    "## 7. Download Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f695056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download checkpoint for local use\n",
    "from google.colab import files  # Works in Kaggle too\n",
    "\n",
    "files.download('weights/hkh_pretrain_v1/best_checkpoint.pth')\n",
    "\n",
    "print(\"\\n✓ Download complete!\")\n",
    "print(\"Upload this file to your GitHub repo or use for competition fine-tuning.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
